```{r setup, include=FALSE}
library(rvest)
library(tidyverse)
library(reshape2)
library(ggplot2)
library(MASS)
library(dplyr)

set.seed(123)

fantasy_cleaned <- read.csv("/Users/oscardecastro/Desktop/fantasy_cleaned.csv")
fantasy_cleaned1 <- as_tibble(fantasy_cleaned, .name_repair = "unique")
fantasy_cleaned1$PPR <- as.numeric(fantasy_cleaned1$PPR)

df_filtered <- fantasy_cleaned1 
df_filtered$G <- as.integer(fantasy_cleaned1$G)
df_filtered <- df_filtered %>% filter(G > 7, FantPos != "FB") 
df_filtered <- df_filtered %>% rename_with(~ make.names(., unique = TRUE))
```

# Density Plot
Visually it appears that with the exception of QB which is normally distributed the other positions are exponentially distributed. We will test wether this is true with Monte Carlo. 
```{r}
ggplot(df_filtered, aes(x = PPR, fill = FantPos)) + 
  geom_density(alpha = 0.5) +  
  labs(
    title = "Density Plot of Groups",
    x = "Value",
    y = "Density"
  ) +
  theme_minimal() +  
  scale_fill_brewer(palette = "Set2")  
```
It is important to mode the differences in data points by position which will influence variability
```{r}
count_by_position <- df_filtered %>%
  group_by(FantPos) %>%
  summarise(Count = n())

print(count_by_position)
```

#Monte Carlo
WR: The p-value is small (<0.05), so the null hypothesis is rejected. The WR data does not fit an exponential distribution well.
RB: The p-value is small (<0.05), so the null hypothesis is rejected. The RB data does not fit an exponential distribution well.
TE: p = 0.065 > 0.05, so we fail to reject the null hypothesis. The TE data appears to fit an exponential distribution reasonably well.
QB: p = 0.124 > 0.05, so we fail to reject the null hypothesis. The QB data appears to fit a normal distribution reasonably well.
```{r}
#Extract observed data for each position
wr_data <- df_filtered %>% filter(FantPos == "WR", !is.na(PPR), is.finite(PPR), PPR >= 0) %>% pull(PPR)
rb_data <- df_filtered %>% filter(FantPos == "RB", !is.na(PPR), is.finite(PPR), PPR >= 0) %>% pull(PPR)
te_data <- df_filtered %>% filter(FantPos == "TE", !is.na(PPR), is.finite(PPR), PPR >= 0) %>% pull(PPR)
qb_data <- df_filtered %>% filter(FantPos == "QB", !is.na(PPR), is.finite(PPR), PPR >= 0) %>% pull(PPR)

#Fit Distributions
wr_fit <- fitdistr(wr_data, "exponential")
rb_fit <- fitdistr(rb_data, "exponential")
te_fit <- fitdistr(te_data, "exponential")
qb_fit <- fitdistr(qb_data, "normal")

#Kolmogorov-Smirnov Tests
wr_p_value <- ks.test(wr_data, "pexp", wr_fit$estimate)
rb_p_value <- ks.test(rb_data, "pexp", rb_fit$estimate)
te_p_value <- ks.test(te_data, "pexp", te_fit$estimate)
qb_p_value <- ks.test(qb_data, "pnorm", mean = qb_fit$estimate[1], sd = qb_fit$estimate[2])

cat("Kolmogorov-Smirnov Test P-Values:\n")
cat("WR:", wr_p_value$p.value, "\n")
cat("RB:", rb_p_value$p.value, "\n")
cat("TE:", te_p_value$p.value, "\n")
cat("QB:", qb_p_value$p.value, "\n")

#Monte Carlo Simulation Function
monte_carlo_cv <- function(n_sim, wr_params, rb_params, te_params, qb_params) {
  cv_values <- list(WR = numeric(n_sim), RB = numeric(n_sim), TE = numeric(n_sim), QB = numeric(n_sim))
  
  for (i in 1:n_sim) {
    # Draw samples based on fitted distributions
    wr_sample <- rexp(100, rate = wr_params)
    rb_sample <- rexp(100, rate = rb_params)
    te_sample <- rexp(100, rate = te_params)
    qb_sample <- rnorm(100, mean = qb_params[1], sd = qb_params[2])
    
    # Calculate CV for each position
    cv_values$WR[i] <- sd(wr_sample) / mean(wr_sample)
    cv_values$RB[i] <- sd(rb_sample) / mean(rb_sample)
    cv_values$TE[i] <- sd(te_sample) / mean(te_sample)
    cv_values$QB[i] <- sd(qb_sample) / mean(qb_sample)
  }
  
  return(cv_values)
}

#Run Monte Carlo Simulation
n_sim <- 10000
cv_results <- monte_carlo_cv(
  n_sim,
  wr_params = wr_fit$estimate,
  rb_params = rb_fit$estimate,
  te_params = te_fit$estimate,
  qb_params = qb_fit$estimate
)

# Visual Results
cv_data <- data.frame(
  CV = c(cv_results$WR, cv_results$RB, cv_results$TE, cv_results$QB),
  Position = rep(c("WR", "RB", "TE", "QB"), each = n_sim)
)

ggplot(cv_data, aes(x = CV, fill = Position)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Monte Carlo Simulated CV Distribution by Position",
    x = "Coefficient of Variation",
    y = "Density"
  ) +
  theme_minimal()
```

#Bootstrap - Stratified by position 
We can see that there is no similarity between positions and their respective CV values compared to one another. We used bootstrap as it is not as dependent on variability as Monte Carlo. 
```{r}
# Stratified Permutation Test Function
stratified_permutation_test <- function(data, group_var, value_var, n_permutations = 1000) {
  # Compute observed differences in CV means across groups
  observed_diff <- tapply(data[[value_var]], data[[group_var]], mean, na.rm = TRUE) %>% diff()
  
  perm_diffs <- numeric(n_permutations)
  
  for (i in 1:n_permutations) {
    # Shuffle values within each group (stratified shuffling)
    permuted_data <- data %>%
      group_by_at(group_var) %>%
      mutate(!!value_var := sample(!!sym(value_var))) %>%
      ungroup()
    
    # Compute permuted differences in means
    permuted_diff <- tapply(permuted_data[[value_var]], permuted_data[[group_var]], mean, na.rm = TRUE) %>% diff()
    perm_diffs[i] <- permuted_diff
  }
  
  # Calculate p-value as the proportion of permuted differences more extreme than observed
  p_value <- mean(abs(perm_diffs) >= abs(observed_diff))
  
  return(list(observed_diff = observed_diff, p_value = p_value))
}

# Bootstrap CV Function
bootstrap_cv <- function(data, n_bootstrap = 1000) {
  boot_cvs <- numeric(n_bootstrap)
  
  for (i in 1:n_bootstrap) {
    sample_data <- data %>% sample_frac(replace = TRUE)  # Sample with replacement
    mean_PPR <- mean(sample_data$PPR, na.rm = TRUE)
    sd_PPR <- sd(sample_data$PPR, na.rm = TRUE)
    boot_cvs[i] <- sd_PPR / mean_PPR
  }
  
  return(boot_cvs)
}

# Generate Bootstrap CV for Positions
wr_data <- df_filtered %>% filter(FantPos == "WR")
wr_bootstrap <- bootstrap_cv(wr_data)

rb_data <- df_filtered %>% filter(FantPos == "RB")
rb_bootstrap <- bootstrap_cv(rb_data)

te_data <- df_filtered %>% filter(FantPos == "TE")
te_bootstrap <- bootstrap_cv(te_data)

qb_data <- df_filtered %>% filter(FantPos == "QB")
qb_bootstrap <- bootstrap_cv(qb_data)

# Combine Bootstrap Results
boot_data <- data.frame(
  CV = c(wr_bootstrap, rb_bootstrap, te_bootstrap, qb_bootstrap),
  Position = factor(rep(c("WR", "RB", "TE", "QB"), 
                        times = c(length(wr_bootstrap), length(rb_bootstrap), 
                                  length(te_bootstrap), length(qb_bootstrap))))
)

# Perform Pairwise T-Test
pairwise_result <- pairwise.t.test(
  boot_data$CV,
  boot_data$Position,
  p.adjust.method = "bonferroni"
)
print(pairwise_result)

calculate_ci <- function(bootstrap_cvs, conf_level = 0.95) {
  alpha <- 1 - conf_level
  ci_lower <- quantile(bootstrap_cvs, probs = alpha / 2, na.rm = TRUE)
  ci_upper <- quantile(bootstrap_cvs, probs = 1 - alpha / 2, na.rm = TRUE)
  return(c(Lower = ci_lower, Upper = ci_upper))
}

# Calculate CIs for each position
wr_ci <- calculate_ci(wr_bootstrap)
rb_ci <- calculate_ci(rb_bootstrap)
te_ci <- calculate_ci(te_bootstrap)
qb_ci <- calculate_ci(qb_bootstrap)

# Print Results
cat("Confidence Intervals for Bootstrap CV:\n")
cat("WR:", wr_ci, "\n")
cat("RB:", rb_ci, "\n")
cat("TE:", te_ci, "\n")
cat("QB:", qb_ci, "\n")

# Visualize Bootstrap CV Distributions
ggplot(boot_data, aes(x = CV, fill = Position)) + 
  geom_density(alpha = 0.5) + 
  labs(title = "Bootstrapped CV Distribution by Position", 
       x = "Coefficient of Variation", 
       y = "Density") +
  theme_minimal()
```

#Jackknife - measure the precision of the Bootstrap estimates 
TE has the most precise bootstrap CV estimates due to its smallest variance, followed by RB, QB, and WR. The higher variance for WR and QB may indicate greater variability in the data for these positions or less predictable patterns in their distributions.
```{r}
# 5-Fold Cross-Validation Function for Bootstrap CV Precision
five_fold_cv_precision <- function(data, group_var, n_bootstrap = 1000, n_folds = 5) {
  # Split the data by position (group_var)
  grouped_data <- split(data, data[[group_var]])
  
  results <- list()
  
  # Iterate over each group (position)
  for (group in names(grouped_data)) {
    group_data <- grouped_data[[group]]
    n <- nrow(group_data)
    
    # Create fold indices
    fold_indices <- sample(rep(1:n_folds, length.out = n))
    fold_estimates <- numeric(n_folds)
    
    # Perform 5-fold cross-validation
    for (fold in 1:n_folds) {
      # Split into train and test sets
      train_data <- group_data[fold_indices != fold, ]
      
      # Compute Bootstrap CV for the Train Set
      bootstrap_cvs <- numeric(n_bootstrap)
      for (j in 1:n_bootstrap) {
        sample_data <- sample(train_data$PPR, replace = TRUE)  # Bootstrap sampling
        bootstrap_cvs[j] <- sd(sample_data, na.rm = TRUE) / mean(sample_data, na.rm = TRUE)  # Compute CV
      }
      
      # Calculate the mean CV for this fold
      fold_estimates[fold] <- mean(bootstrap_cvs, na.rm = TRUE)
    }
    
    # Calculate the variance of the fold estimates (Precision of Bootstrap CV)
    cross_validation_variance <- var(fold_estimates, na.rm = TRUE)
    
    results[[group]] <- list(
      fold_estimates = fold_estimates,
      cross_validation_variance = cross_validation_variance
    )
  }
  
  return(results)
}

# Apply 5-Fold CV Precision Function
kfold_results <- five_fold_cv_precision(df_filtered, group_var = "FantPos", n_bootstrap = 1000, n_folds = 5)

for (position in names(kfold_results)) {
  cat("Position:", position, "\n")
  cat("5-Fold Variance of Bootstrap CV:", kfold_results[[position]]$cross_validation_variance, "\n\n")
}

#Combine Results for Visualization
kfold_data <- do.call(rbind, lapply(names(kfold_results), function(pos) {
  data.frame(
    Position = pos,
    Fold_Estimates = kfold_results[[pos]]$fold_estimates
  )
}))

ggplot(kfold_data, aes(x = Fold_Estimates, fill = Position)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "5-Fold CV Precision of Bootstrap CV by Position",
    x = "Bootstrap CV Estimate",
    y = "Density"
  ) + theme_minimal() + xlim(0.45,1)
```

#Other Important Variables 
```{r}
anova1 <- aov(PPR ~ FantPos + Age + Tm, data = fantasy_cleaned1)
summary(anova1)
```
